# -*- coding: utf-8 -*-
"""Causal connectives.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13SIsp7nZTlRjMDWxTA84xqslS5MFNXfp

## Collect connective sentence and its previous and following sentence
"""

# Starting with installations

!git clone https://github.com/explosion/spaCy.git
!git clone https://github.com/explosion/spacy-transformers.git

import spacy.cli
spacy.cli.download("nl")

!pwd

cd drive

cd My\ Drive

cd Colab\ Notebooks

# example of the dataset

!head OpenSubtitles.en-nl.nl

# opening the database

with open('OpenSubtitles.en-nl.nl', 'r') as f: 
    ennl = f.read()

# loading the nlp model

nlp = spacy.load('nl_core_news_sm')

# seperate into three pieces of 20.000 sentences, because the nlp model can't take more than 20.000 sentences at once

ennl1 = ennl.splitlines()[0:20000]
ennl1_str = " ".join(ennl1)
en_nl_1 = nlp(ennl1_str)
en_nl_1_sents = list(en_nl_1.sents)

ennl2 = ennl.splitlines()[20000:40000]
ennl2_str = " ".join(ennl2)
en_nl_2 = nlp(ennl2_str)
en_nl_2_sents = list(en_nl_2.sents)

ennl3 = ennl.splitlines()[40000:60000]
ennl3_str = " ".join(ennl3)
en_nl_3 = nlp(ennl3_str)
en_nl_3_sents = list(en_nl_3.sents)

# Finding the sentences containing the connectives using the nlp model


def find_matches(doc, target):

  matches = []

  for i, sent in enumerate(doc):
    match_found = False
    for token in sent:
      if token.text.lower() == target:
        match_found = True
        break

    if match_found:
      matches.append(doc[i-1: i+2])

  concatenates = []

  for sentences in matches:
    for i, sentence in enumerate(sentences):
      sentences[i] = str(sentence)

  for item in matches:
    concatenates.append(' '.join(item))

  return concatenates


 
dus_matches = find_matches(en_nl_1_sents, 'dus') +  find_matches(en_nl_2_sents, 'dus') + find_matches(en_nl_3_sents, 'dus')
daarom_matches = find_matches(en_nl_1_sents, 'daarom') +  find_matches(en_nl_2_sents, 'daarom') + find_matches(en_nl_3_sents, 'daarom')
daardoor_matches = find_matches(en_nl_1_sents, 'daardoor') +  find_matches(en_nl_2_sents, 'daardoor') + find_matches(en_nl_3_sents, 'daardoor')
omdat_matches = find_matches(en_nl_1_sents, 'omdat') +  find_matches(en_nl_2_sents, 'omdat') + find_matches(en_nl_3_sents, 'omdat')
want_matches = find_matches(en_nl_1_sents, 'want') +  find_matches(en_nl_2_sents, 'want') + find_matches(en_nl_3_sents, 'want')


print('Amount of sentences containing "dus":      ',len(dus_matches))
print('Amount of sentences containing "daarom":   ',len(daarom_matches))
print('Amount of sentences containing "daardoor": ',len(daardoor_matches))
print('Amount of sentences containing "omdat":    ',len(omdat_matches))
print('Amount of sentences containing "want":     ',len(want_matches))

"""## Neccessary installations and defenitions for working with the Bertje model"""

# needed instalations

!pip install transformers

from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained("bert-base-dutch-cased")
model = BertModel.from_pretrained("bert-base-dutch-cased")

# code for encoding sentences with Bertje

import torch

def encode_sentence(sentence):
    input_ids = torch.tensor(
        [tokenizer.encode(sentence, add_special_tokens=True)])
    with torch.no_grad():
        last_hidden_states = model(input_ids)[0]
    return last_hidden_states[0].numpy()

"""## Creating list of vectors for connectives"""

# adding the right labels which are required for the model

def label_sents(doc):
  labelled_sents = []
  for query in doc:
    labelled_sents.append("[CLS] " + query + " [SEP]")
  return labelled_sents

labelled_sents_dus = label_sents(dus_matches)
labelled_sents_daarom= label_sents(daarom_matches)
labelled_sents_daardoor = label_sents(daardoor_matches)
labelled_sents_omdat = label_sents(omdat_matches)
labelled_sents_want = label_sents(want_matches)


#for i, x in enumerate(labelled_sents_daardoor):
#  print(i, x)

# splitting the labelled sentences in tokens

def tokenize_sents(doc):
  
  tokenized_sents = []
  for x in doc:
    y = tokenizer.tokenize(x)
    tokenized_sents.append(y)

  return tokenized_sents 

tokenized_sents_dus = tokenize_sents(labelled_sents_dus)
tokenized_sents_daarom = tokenize_sents(labelled_sents_daarom)
tokenized_sents_daardoor = tokenize_sents(labelled_sents_daardoor)
tokenized_sents_omdat = tokenize_sents(labelled_sents_omdat)
tokenized_sents_want = tokenize_sents(labelled_sents_want)


#print(tokenized_sents_daardoor)

# creating a list with the indexes of the connective in the sentences

def create_index_list(doc, target):

  index_list = []
  for nummer, x in enumerate(doc):
   for i, woord in enumerate(x):
    if woord.lower() == target:
      index_list.append( (nummer, i) )
  
  return index_list


dus_index_list = create_index_list(tokenized_sents_dus, 'dus')
daarom_index_list = create_index_list(tokenized_sents_daarom, 'daarom')

omdat_index_list = create_index_list(tokenized_sents_omdat, 'omdat')
want_index_list = create_index_list(tokenized_sents_want, 'want')


# 'Daardoor' needs separate code because it is sometimes split into two parts depending on upper/lower case

daardoor_index_list = []

for nummer, x in enumerate(tokenized_sents_daardoor):
   for i, woord in enumerate(x):
    if woord.lower() == 'daar' and x[i+1] == '##door':
      daardoor_index_list.append( (nummer, i) )
    elif woord.lower() == 'daardoor':
      daardoor_index_list.append( (nummer, i) )

#print(dus_index_list)

# creating a default dictonary with the indexes

from collections import defaultdict

def create_default_dict(index_list):

  default_dict = defaultdict(list)

  for key, value in index_list:
    default_dict[key].append(value)
  
  return default_dict


dus_default_dict = create_default_dict(dus_index_list)
daarom_default_dict = create_default_dict(daarom_index_list)
daardoor_default_dict = create_default_dict(daardoor_index_list)
omdat_default_dict = create_default_dict(omdat_index_list)
want_default_dict = create_default_dict(want_index_list)

# encoding the sentences (takes some time)

encoded_sents_dus = [encode_sentence(sent) for sent in tokenized_sents_dus]
encoded_sents_daarom = [encode_sentence(sent) for sent in tokenized_sents_daarom]
encoded_sents_daardoor = [encode_sentence(sent) for sent in tokenized_sents_daardoor]
encoded_sents_omdat = [encode_sentence(sent) for sent in tokenized_sents_omdat]
encoded_sents_want = [encode_sentence(sent) for sent in tokenized_sents_want]

# creating the list with vectors of the connectives

def find_vecs(encoded_sents, default_dict):

  vecs = []
  doubles_dict = {}

  for i, x in enumerate(encoded_sents):
    values = default_dict[i]
    for value in values:
      vecs.append(x[value])
    if len(values) != 1:
      doubles_dict.update({i:len(values)})
      print('Watch out: in sentence ', i, ' contains ', len(values), ' instances of the connective.')
  
  return vecs, doubles_dict

print('For "dus" holds the following warnings: ')
dus_vecs, dus_doubles = find_vecs(encoded_sents_dus, dus_default_dict)

print('\nFor "daarom" holds the following warnings: ')
daarom_vecs, daarom_doubles = find_vecs(encoded_sents_daarom, daarom_default_dict)

print('\nFor "daardoor" holds the following warnings: ')
daardoor_vecs, daardoor_doubles = find_vecs(encoded_sents_daardoor, daardoor_default_dict)

print('\nFor "omdat" holds the following warnings: ')
omdat_vecs, omdat_doubles = find_vecs(encoded_sents_omdat, omdat_default_dict)

print('\nFor "want" holds the following warnings: ')
want_vecs, want_doubles = find_vecs(encoded_sents_want, want_default_dict)


# If the context surrounding the target sentence contains a connective a vector for this is also added to the list of vectors
# This not necessary since the connective is already saved in its own target sentence and therefore already has a vector

print(want_doubles)

# Filtering out the double connectives (sadly no other option than to count by hand)
# (we don't want the vectors of connectives in the context sentences, but only those from the target sentences)

#for i, sentence in enumerate(want_matches):
#  if i in (want_doubles.keys()):
#    print(i, sentence)


# indexes of vecs which need to be filtered out from dus_vecs
dus_excess = [16, 17, 19, 20, 22, 23, 25, 26, 32, 33, 50, 51, 57, 58, 127, 128, 171, 172, 300, 301, 265, 266 ]
omdat_excess = [127, 128, 148, 149, 184, 185, 187, 188, 225, 226 ]
want_excess = [8, 9, 93, 94]

# Removing excess vectors from vecs


dus_vecs_new = []

for i, vec in enumerate(dus_vecs):
  if i not in dus_excess:
    dus_vecs_new.append(vec)


omdat_vecs_new = []

for i, vec in enumerate(omdat_vecs):
  if i not in omdat_excess:
    omdat_vecs_new.append(vec)



want_vecs_new = []

for i, vec in enumerate(want_vecs):
  if i not in want_excess:
    want_vecs_new.append(vec)


omdat_vecs = omdat_vecs_new
want_vecs = want_vecs_new
dus_vecs = dus_vecs_new

# Creating lists of sentences matching with the vecs, so target sentences with two vecs are doubled

matching_sents_dus = []

for i, item in enumerate(dus_matches):
  if i != 5:
    matching_sents_dus.append(item)
  elif i == 5:
    matching_sents_dus.append(item)
    matching_sents_dus.append(item)

matching_sents_omdat = []

for i, item in enumerate(omdat_matches):
  if i != 41:
    if i != 150:
      matching_sents_omdat.append(item)
  if i == 41:
    matching_sents_omdat.append(item)
    matching_sents_omdat.append(item)
  if i == 150:
      matching_sents_omdat.append(item)
      matching_sents_omdat.append(item)

matching_sents_daarom = daarom_matches
matching_sents_daardoor = daardoor_matches
matching_sents_want = want_matches


matching_sents_all = matching_sents_dus + matching_sents_daarom + matching_sents_daardoor + matching_sents_omdat + matching_sents_want
matching_sents_dus_daarom_daardoor =  matching_sents_dus + matching_sents_daarom + matching_sents_daardoor
matching_sents_omdat_want = matching_sents_omdat + matching_sents_want

"""## Turning the lists of vecs into 2D representations"""

# Commented out IPython magic to ensure Python compatibility.
# starting with necessary installations

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patheffects as PathEffects
# %matplotlib inline

import seaborn as sns
sns.set_style('darkgrid')
sns.set_palette('muted')
sns.set_context("notebook", font_scale=1.5,
                rc={"lines.linewidth": 2.5})
RS = 42

# turning the lists of vecs into data frames

import pandas as pd

dus_vecs_df = pd.DataFrame(dus_vecs)
daarom_vecs_df = pd.DataFrame(daarom_vecs)
daardoor_vecs_df = pd.DataFrame(daardoor_vecs)
omdat_vecs_df = pd.DataFrame(omdat_vecs)
want_vecs_df = pd.DataFrame(want_vecs)


#print (omdat_vecs_df[0]) # print kolom 0

type(omdat_vecs_df[0]) # labels moet ook series worden

# Combining vector lists for the combinations and turning those into data frames



omdat_want_vecs = omdat_vecs + want_vecs
dus_daarom_daardoor_vecs = dus_vecs + daarom_vecs + daardoor_vecs
dus_daarom_daardoor_omdat_want_vecs = dus_daarom_daardoor_vecs + omdat_want_vecs


omdat_want_vecs_df = pd.DataFrame(omdat_want_vecs)
dus_daarom_daardoor_vecs_df = pd.DataFrame(dus_daarom_daardoor_vecs)
dus_daarom_daardoor_omdat_want_vecs_df = pd.DataFrame(dus_daarom_daardoor_omdat_want_vecs)

# code for the plot

def labeled_scatter(x, labels, size):
  
  # choose a color palette with seaborn.
  num_classes = len(np.unique(labels))
  print(num_classes) # kan straks weg
  palette = np.array(sns.color_palette("hls", num_classes))

  colors = np.zeros(len(labels), dtype= object)
  for i, label in enumerate(set(labels)):
    colors[labels == label] = f'C{i}'

  # create a scatter plot.
  f = plt.figure(figsize=(size, size))
  ax = plt.subplot(aspect='equal')
  sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=colors)
  plt.xlim(-25, 25)
  plt.ylim(-25, 25)
  ax.axis('off')
  ax.axis('tight')

  # add the labels for each digit corresponding to the label
  txts = []

  for i in range(len(x)):

    # Position of each label at median of data po

    plt.annotate(i, xy = (x[i,0], x[i,1]))

  return f, ax, sc, txts

# Creating color coordinated labels

# seperate
labels_dus = ['dus'] * len(dus_vecs)
labels_daarom = ['daarom'] * len(daarom_vecs)
labels_daardoor = ['daardoor'] * len(daardoor_vecs)
labels_omdat = ['omdat'] * len(omdat_vecs)
labels_want = ['want'] * len(want_vecs)

# combinations
labels_omdat_want = labels_omdat + labels_want
labels_dus_daarom_daardoor = labels_dus + labels_daarom + labels_daardoor
labels_dus_daarom_daardoor_omdat_want = labels_dus_daarom_daardoor + labels_omdat_want

# putting the labels in the right format
labels_dus_pd = pd.Series(labels_dus)
labels_daarom_pd = pd.Series(labels_daarom)
labels_daardoor_pd = pd.Series(labels_daardoor)
labels_omdat_pd = pd.Series(labels_omdat)
labels_want_pd = pd.Series(labels_want)

labels_omdat_want_pd = pd.Series(labels_omdat_want)
labels_dus_daarom_daardoor_pd = pd.Series(labels_dus_daarom_daardoor)
labels_dus_daarom_daardoor_omdat_want_pd = pd.Series(labels_dus_daarom_daardoor_omdat_want)

# Applying PCA

from sklearn.decomposition import PCA

dus_vecs_pca = PCA(n_components=2).fit_transform(dus_vecs_df.values)
daarom_vecs_pca = PCA(n_components=2).fit_transform(daarom_vecs_df.values)
daardoor_vecs_pca = PCA(n_components=2).fit_transform(daardoor_vecs_df.values)
omdat_vecs_pca = PCA(n_components=2).fit_transform(omdat_vecs_df.values)
want_vecs_pca = PCA(n_components=2).fit_transform(want_vecs_df.values)

omdat_want_vecs_pca = PCA(n_components=2).fit_transform(omdat_want_vecs_df.values)
dus_daarom_daardoor_vecs_pca = PCA(n_components=2).fit_transform(dus_daarom_daardoor_vecs_df.values)
dus_daarom_daardoor_omdat_want_vecs_pca = PCA(n_components=2).fit_transform(dus_daarom_daardoor_omdat_want_vecs_df.values)

# making a plot voor 'omdat' and 'want'

labeled_scatter(omdat_want_vecs_pca, labels_omdat_want_pd, 100)

# sents matching to plot of omdat and want

for i, item in enumerate(matching_sents_omdat_want):
  print(i, item)

# making a plot voor 'dus', 'daarom' and 'daardoor'

labeled_scatter(dus_daarom_daardoor_vecs_pca, labels_dus_daarom_daardoor_pd, 100)

# sents matching to plot of dus, daarom, daardoor

for i, item in enumerate(matching_sents_dus_daarom_daardoor):
  print(i, item)

# making a plot voor all the connectives combined

labeled_scatter(dus_daarom_daardoor_omdat_want_vecs_pca, labels_dus_daarom_daardoor_omdat_want_pd, 150)

# sents matching to plot of all the connectives

for i, item in enumerate(matching_sents_all):
  print(i, item)

# making a plot voor all the connectives individually

# dus

labeled_scatter(dus_vecs_pca, labels_dus_pd, 100)

# sents matching to dus plot

for i, item in enumerate(matching_sents_dus):
  print(i, item)

# daarom

labeled_scatter(daarom_vecs_pca, labels_daarom_pd, 100)

# sents matching to daarom plot

for i, item in enumerate(matching_sents_daarom):
  print(i, item)

# daardoor

labeled_scatter(daardoor_vecs_pca, labels_daardoor_pd, 25)

# sents matching to daardoor plot

for i, item in enumerate(matching_sents_daardoor):
  print(i, item)

# want

labeled_scatter(want_vecs_pca, labels_want_pd, 100)

# sents matching to want plot

for i, item in enumerate(matching_sents_want):
  print(i, item)

# omdat

labeled_scatter(omdat_vecs_pca, labels_omdat_pd, 100)

# sents matching to omdat plot

for i, item in enumerate(matching_sents_omdat):
  print(i, item)