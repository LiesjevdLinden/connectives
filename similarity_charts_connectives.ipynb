{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "similarity charts connectives.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1xYKWq_ml7KfbK-yNfYxLtMGyeD-_S1J2",
      "authorship_tag": "ABX9TyO/9UQAy7gLK9md9dUmwNdR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LiesjevdLinden/connectives/blob/master/similarity_charts_connectives.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BitH3A3q31CS",
        "colab_type": "text"
      },
      "source": [
        "## Collect connective sentence and its previous and following sentence\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EaM7dKKSXYV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "4e75c82c-1f34-4396-a594-3f11134cd3bf"
      },
      "source": [
        "# Starting with installations\n",
        "\n",
        "!git clone https://github.com/explosion/spaCy.git\n",
        "!git clone https://github.com/explosion/spacy-transformers.git\n",
        "\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"nl\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'spaCy'...\n",
            "remote: Enumerating objects: 164, done.\u001b[K\n",
            "remote: Counting objects: 100% (164/164), done.\u001b[K\n",
            "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
            "remote: Total 74455 (delta 94), reused 84 (delta 46), pack-reused 74291\u001b[K\n",
            "Receiving objects: 100% (74455/74455), 167.45 MiB | 13.90 MiB/s, done.\n",
            "Resolving deltas: 100% (54538/54538), done.\n",
            "Cloning into 'spacy-transformers'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 5461 (delta 51), reused 77 (delta 39), pack-reused 5364\u001b[K\n",
            "Receiving objects: 100% (5461/5461), 875.40 KiB | 2.28 MiB/s, done.\n",
            "Resolving deltas: 100% (3639/3639), done.\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('nl_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/nl_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/nl\n",
            "You can now load the model via spacy.load('nl')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RII0E853Uwwv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "96e898cd-2e52-49b2-d9a0-44730d0cfb3c"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4Ax6GdDV_lG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "21853945-873f-4328-baa4-896b2ecf21e9"
      },
      "source": [
        "cd drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1RtkX1rWK0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aa67d3e3-4458-4cda-9301-e82700e27792"
      },
      "source": [
        "cd My\\ Drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIFnLbiw3HZ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1dde187b-f0af-4635-c729-cf91a3db4475"
      },
      "source": [
        "cd Colab\\ Notebooks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nu-Nsy3XjhY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "8ee1d208-a94b-48a5-a568-209325b487bd"
      },
      "source": [
        "# example of the dataset\n",
        "\n",
        "!head OpenSubtitles.en-nl.nl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Een dodelijke asteroïde snelt met 107.200 km/u op de aarde af.\n",
            "Gloeiend hete, schoorsteenachtige torens... die verrijzen vanaf de oceaanbodem.\n",
            "De zeereis van 'n jongeman die de wereld op z'n kop zette.\n",
            "We gaan terug in de tijd, op zoek naar de oorsprong van onze voorouders.\n",
            "Van alle wetenschappelijke ontdekkingen... zijn er maar weinig die zo boeiend en controversieel zijn... als die over de oorsprong van het leven op aarde.\n",
            "Hoe soorten evolueerden... waarom sommige soorten overleefden en andere uitstierven.\n",
            "Dit zijn de grootste ontdekkingen over de oorsprong en evolutie van 't leven.\n",
            "Het leven is een kansspel.\n",
            "De kansen op overleving of uitsterving veranderen steeds.\n",
            "Soms gaat dat traag door langzame natuurprocessen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cw-N5BuXrna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# opening the database\n",
        "\n",
        "with open('OpenSubtitles.en-nl.nl', 'r') as f: \n",
        "    ennl = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s9xMKrj__J_",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yuzhz6BLbcAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading the nlp model\n",
        "\n",
        "nlp = spacy.load('nl_core_news_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2_gVGg0aw_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# seperate into three pieces of 20.000 sentences, because the nlp model can't take more than 20.000 sentences at once\n",
        "\n",
        "ennl1 = ennl.splitlines()[0:20000]\n",
        "ennl1_str = \" \".join(ennl1)\n",
        "en_nl_1 = nlp(ennl1_str)\n",
        "en_nl_1_sents = list(en_nl_1.sents)\n",
        "\n",
        "ennl2 = ennl.splitlines()[20000:40000]\n",
        "ennl2_str = \" \".join(ennl2)\n",
        "en_nl_2 = nlp(ennl2_str)\n",
        "en_nl_2_sents = list(en_nl_2.sents)\n",
        "\n",
        "ennl3 = ennl.splitlines()[40000:60000]\n",
        "ennl3_str = \" \".join(ennl3)\n",
        "en_nl_3 = nlp(ennl3_str)\n",
        "en_nl_3_sents = list(en_nl_3.sents)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWj3Sh9qscf0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "b4af7cbc-0d79-4f68-9fb7-5bb74b73d21b"
      },
      "source": [
        "# Finding the sentences containing the connectives using the nlp model\n",
        "\n",
        "\n",
        "def find_matches(doc, target):\n",
        "\n",
        "  matches = []\n",
        "\n",
        "  for i, sent in enumerate(doc):\n",
        "    match_found = False\n",
        "    for token in sent:\n",
        "      if token.text.lower() == target:\n",
        "        match_found = True\n",
        "        break\n",
        "\n",
        "    if match_found:\n",
        "      matches.append(doc[i-1: i+2])\n",
        "\n",
        "  concatenates = []\n",
        "\n",
        "  for sentences in matches:\n",
        "    for i, sentence in enumerate(sentences):\n",
        "      sentences[i] = str(sentence)\n",
        "\n",
        "  for item in matches:\n",
        "    concatenates.append(' '.join(item))\n",
        "\n",
        "  return concatenates\n",
        "\n",
        "\n",
        " \n",
        "dus_matches = find_matches(en_nl_1_sents, 'dus') +  find_matches(en_nl_2_sents, 'dus') + find_matches(en_nl_3_sents, 'dus')\n",
        "daarom_matches = find_matches(en_nl_1_sents, 'daarom') +  find_matches(en_nl_2_sents, 'daarom') + find_matches(en_nl_3_sents, 'daarom')\n",
        "daardoor_matches = find_matches(en_nl_1_sents, 'daardoor') +  find_matches(en_nl_2_sents, 'daardoor') + find_matches(en_nl_3_sents, 'daardoor')\n",
        "omdat_matches = find_matches(en_nl_1_sents, 'omdat') +  find_matches(en_nl_2_sents, 'omdat') + find_matches(en_nl_3_sents, 'omdat')\n",
        "want_matches = find_matches(en_nl_1_sents, 'want') +  find_matches(en_nl_2_sents, 'want') + find_matches(en_nl_3_sents, 'want')\n",
        "\n",
        "\n",
        "print('Amount of sentences containing \"dus\":      ',len(dus_matches))\n",
        "print('Amount of sentences containing \"daarom\":   ',len(daarom_matches))\n",
        "print('Amount of sentences containing \"daardoor\": ',len(daardoor_matches))\n",
        "print('Amount of sentences containing \"omdat\":    ',len(omdat_matches))\n",
        "print('Amount of sentences containing \"want\":     ',len(want_matches))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Amount of sentences containing \"dus\":       512\n",
            "Amount of sentences containing \"daarom\":    74\n",
            "Amount of sentences containing \"daardoor\":  16\n",
            "Amount of sentences containing \"omdat\":     228\n",
            "Amount of sentences containing \"want\":      119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzNQXSzSnnOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Also create lists containing the sentences without connectives\n",
        "\n",
        "def delete_punctuation(matches):\n",
        "\n",
        "  new_matches = []\n",
        "\n",
        "  for match in matches:\n",
        "    new_match = match.replace(\".\", \"\")\n",
        "    new_match1 = new_match.replace(\",\", \"\")\n",
        "    new_match2 = new_match1.replace(\"-\", \"\")\n",
        "    new_match3 = new_match2.replace(\"?\", \"\")\n",
        "    new_matches.append(new_match3)\n",
        "\n",
        "  return new_matches\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMTVflK042ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dus_no_punctuation = delete_punctuation(dus_matches)\n",
        "daarom_no_punctuation = delete_punctuation(daarom_matches)\n",
        "daardoor_no_punctuation = delete_punctuation(daardoor_matches)\n",
        "omdat_no_punctuation = delete_punctuation(omdat_matches)\n",
        "want_no_punctuation = delete_punctuation(want_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxkE8G6l4YMo",
        "colab_type": "text"
      },
      "source": [
        "## Neccessary installations and defenitions for working with the Bertje model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anDaD0JnFEfe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "8753f684-1dbd-4c06-ee09-62a966c663a3"
      },
      "source": [
        "# needed instalations\n",
        "\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmKdCk2DqjuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP8OpzVgIHNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"wietsedv/bert-base-dutch-cased\")\n",
        "model = BertModel.from_pretrained(\"wietsedv/bert-base-dutch-cased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXc4uK3DIVkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for encoding sentences with Bertje\n",
        "\n",
        "import torch\n",
        "\n",
        "def encode_sentence(sentence):\n",
        "    input_ids = torch.tensor(\n",
        "        [tokenizer.encode(sentence, add_special_tokens=True)])\n",
        "    with torch.no_grad():\n",
        "        last_hidden_states = model(input_ids)[0]\n",
        "    return last_hidden_states[0].numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46neSsUVL2Kl",
        "colab_type": "text"
      },
      "source": [
        "## Creating list of vectors for connectives "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GBwd_PIJgNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adding the right labels which are required for the model\n",
        "\n",
        "def label_sents(doc):\n",
        "  labelled_sents = []\n",
        "  for query in doc:\n",
        "    labelled_sents.append(\"[CLS] \" + query + \" [SEP]\")\n",
        "  return labelled_sents\n",
        "\n",
        "labelled_sents_dus = label_sents(dus_matches)\n",
        "labelled_sents_daarom= label_sents(daarom_matches)\n",
        "labelled_sents_daardoor = label_sents(daardoor_matches)\n",
        "labelled_sents_omdat = label_sents(omdat_matches)\n",
        "labelled_sents_want = label_sents(want_matches)\n",
        "\n",
        "\n",
        "# doing the same for the ones without punctuation\n",
        "\n",
        "labelled_sents_dus_no_punctuation = label_sents(dus_no_punctuation)\n",
        "labelled_sents_daarom_no_punctuation= label_sents(daarom_no_punctuation)\n",
        "labelled_sents_daardoor_no_punctuation = label_sents(daardoor_no_punctuation)\n",
        "labelled_sents_omdat_no_punctuation = label_sents(omdat_no_punctuation)\n",
        "labelled_sents_want_no_punctuation = label_sents(want_no_punctuation)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7Mkjx5sKH4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# splitting the labelled sentences in tokens\n",
        "\n",
        "def tokenize_sents(doc):\n",
        "  \n",
        "  tokenized_sents = []\n",
        "  for x in doc:\n",
        "    y = tokenizer.tokenize(x)\n",
        "    tokenized_sents.append(y)\n",
        "\n",
        "  return tokenized_sents \n",
        "\n",
        "tokenized_sents_dus = tokenize_sents(labelled_sents_dus)\n",
        "tokenized_sents_daarom = tokenize_sents(labelled_sents_daarom)\n",
        "tokenized_sents_daardoor = tokenize_sents(labelled_sents_daardoor)\n",
        "tokenized_sents_omdat = tokenize_sents(labelled_sents_omdat)\n",
        "tokenized_sents_want = tokenize_sents(labelled_sents_want)\n",
        "\n",
        "\n",
        "# doing the same for the ones without punctuation\n",
        "\n",
        "tokenized_sents_dus_no_punctuation = tokenize_sents(labelled_sents_dus_no_punctuation)\n",
        "tokenized_sents_daarom_no_punctuation = tokenize_sents(labelled_sents_daarom_no_punctuation)\n",
        "tokenized_sents_daardoor_no_punctuation = tokenize_sents(labelled_sents_daardoor_no_punctuation)\n",
        "tokenized_sents_omdat_no_punctuation = tokenize_sents(labelled_sents_omdat_no_punctuation)\n",
        "tokenized_sents_want_no_punctuation = tokenize_sents(labelled_sents_want_no_punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgF46U_cKjOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a list with the indexes of the connective in the sentences\n",
        "\n",
        "def create_index_list(doc, target):\n",
        "\n",
        "  index_list = []\n",
        "  for nummer, x in enumerate(doc):\n",
        "   for i, woord in enumerate(x):\n",
        "    if woord.lower() == target:\n",
        "      index_list.append( (nummer, i) )\n",
        "  \n",
        "  return index_list\n",
        "\n",
        "\n",
        "dus_index_list = create_index_list(tokenized_sents_dus, 'dus')\n",
        "daarom_index_list = create_index_list(tokenized_sents_daarom, 'daarom')\n",
        "\n",
        "omdat_index_list = create_index_list(tokenized_sents_omdat, 'omdat')\n",
        "want_index_list = create_index_list(tokenized_sents_want, 'want')\n",
        "\n",
        "\n",
        "# 'Daardoor' needs separate code because it is sometimes split into two parts depending on upper/lower case\n",
        "\n",
        "daardoor_index_list = []\n",
        "\n",
        "for nummer, x in enumerate(tokenized_sents_daardoor):\n",
        "   for i, woord in enumerate(x):\n",
        "    if woord.lower() == 'daar' and x[i+1] == '##door':\n",
        "      daardoor_index_list.append( (nummer, i) )\n",
        "    elif woord.lower() == 'daardoor':\n",
        "      daardoor_index_list.append( (nummer, i) )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Doing the same for the ones without puntuation\n",
        "\n",
        "dus_index_list_no_punctuation = create_index_list(tokenized_sents_dus_no_punctuation, 'dus')\n",
        "daarom_index_list_no_punctuation = create_index_list(tokenized_sents_daarom_no_punctuation, 'daarom')\n",
        "\n",
        "omdat_index_list_no_punctuation = create_index_list(tokenized_sents_omdat_no_punctuation, 'omdat')\n",
        "want_index_list_no_punctuation = create_index_list(tokenized_sents_want_no_punctuation, 'want')\n",
        "\n",
        "daardoor_index_list_no_punctuation = []\n",
        "\n",
        "for nummer, x in enumerate(tokenized_sents_daardoor_no_punctuation):\n",
        "   for i, woord in enumerate(x):\n",
        "    if woord.lower() == 'daar' and x[i+1] == '##door':\n",
        "      daardoor_index_list_no_punctuation.append( (nummer, i) )\n",
        "    elif woord.lower() == 'daardoor':\n",
        "      daardoor_index_list_no_punctuation.append( (nummer, i) )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asg4PonPLDZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a default dictonary with the indexes\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def create_default_dict(index_list):\n",
        "\n",
        "  default_dict = defaultdict(list)\n",
        "\n",
        "  for key, value in index_list:\n",
        "    default_dict[key].append(value)\n",
        "  \n",
        "  return default_dict\n",
        "\n",
        "\n",
        "dus_default_dict = create_default_dict(dus_index_list)\n",
        "daarom_default_dict = create_default_dict(daarom_index_list)\n",
        "daardoor_default_dict = create_default_dict(daardoor_index_list)\n",
        "omdat_default_dict = create_default_dict(omdat_index_list)\n",
        "want_default_dict = create_default_dict(want_index_list)\n",
        "\n",
        "\n",
        "# Doing the same for the ones without puntuation\n",
        "\n",
        "dus_default_dict_no_punctuation = create_default_dict(dus_index_list_no_punctuation)\n",
        "daarom_default_dict_no_punctuation = create_default_dict(daarom_index_list_no_punctuation)\n",
        "daardoor_default_dict_no_punctuation = create_default_dict(daardoor_index_list_no_punctuation)\n",
        "omdat_default_dict_no_punctuation = create_default_dict(omdat_index_list_no_punctuation)\n",
        "want_default_dict_no_punctuation = create_default_dict(want_index_list_no_punctuation)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up1Zj6P0LUiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoding the sentences (takes some time)\n",
        "\n",
        "encoded_sents_dus = [encode_sentence(sent) for sent in tokenized_sents_dus]\n",
        "encoded_sents_daarom = [encode_sentence(sent) for sent in tokenized_sents_daarom]\n",
        "encoded_sents_daardoor = [encode_sentence(sent) for sent in tokenized_sents_daardoor]\n",
        "encoded_sents_omdat = [encode_sentence(sent) for sent in tokenized_sents_omdat]\n",
        "encoded_sents_want = [encode_sentence(sent) for sent in tokenized_sents_want]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmV_sh6lCohW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoding the sentences without punctuation\n",
        "\n",
        "encoded_sents_dus_no_punctuation = [encode_sentence(sent) for sent in tokenized_sents_dus_no_punctuation]\n",
        "encoded_sents_daarom_no_punctuation = [encode_sentence(sent) for sent in tokenized_sents_daarom_no_punctuation]\n",
        "encoded_sents_daardoor_no_punctuation = [encode_sentence(sent) for sent in tokenized_sents_daardoor_no_punctuation]\n",
        "encoded_sents_omdat_no_punctuation = [encode_sentence(sent) for sent in tokenized_sents_omdat_no_punctuation]\n",
        "encoded_sents_want_no_punctuation = [encode_sentence(sent) for sent in tokenized_sents_want_no_punctuation]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fyShWmfMDlg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "cf901ec9-debd-4c1e-9d8e-a02ba8fe6e2d"
      },
      "source": [
        "# creating the list with vectors of the connectives\n",
        "\n",
        "def find_vecs(encoded_sents, default_dict):\n",
        "\n",
        "  vecs = []\n",
        "  doubles_dict = {}\n",
        "\n",
        "  for i, x in enumerate(encoded_sents):\n",
        "    values = default_dict[i]\n",
        "    for value in values:\n",
        "      vecs.append(x[value])\n",
        "    if len(values) != 1:\n",
        "      doubles_dict.update({i:len(values)})\n",
        "      print('Watch out: in sentence ', i, ' contains ', len(values), ' instances of the connective.')\n",
        "  \n",
        "  return vecs, doubles_dict\n",
        "\n",
        "print('For \"dus\" holds the following warnings: ')\n",
        "dus_vecs, dus_doubles = find_vecs(encoded_sents_dus, dus_default_dict)\n",
        "\n",
        "print('\\nFor \"daarom\" holds the following warnings: ')\n",
        "daarom_vecs, daarom_doubles = find_vecs(encoded_sents_daarom, daarom_default_dict)\n",
        "\n",
        "print('\\nFor \"daardoor\" holds the following warnings: ')\n",
        "daardoor_vecs, daardoor_doubles = find_vecs(encoded_sents_daardoor, daardoor_default_dict)\n",
        "\n",
        "print('\\nFor \"omdat\" holds the following warnings: ')\n",
        "omdat_vecs, omdat_doubles = find_vecs(encoded_sents_omdat, omdat_default_dict)\n",
        "\n",
        "print('\\nFor \"want\" holds the following warnings: ')\n",
        "want_vecs, want_doubles = find_vecs(encoded_sents_want, want_default_dict)\n",
        "\n",
        "\n",
        "# If the context surrounding the target sentence contains a connective a vector for this is also added to the list of vectors\n",
        "# This not necessary since the connective is already saved in its own target sentence and therefore already has a vector\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For \"dus\" holds the following warnings: \n",
            "Watch out: in sentence  5  contains  2  instances of the connective.\n",
            "Watch out: in sentence  14  contains  2  instances of the connective.\n",
            "Watch out: in sentence  15  contains  3  instances of the connective.\n",
            "Watch out: in sentence  16  contains  3  instances of the connective.\n",
            "Watch out: in sentence  17  contains  3  instances of the connective.\n",
            "Watch out: in sentence  18  contains  2  instances of the connective.\n",
            "Watch out: in sentence  22  contains  2  instances of the connective.\n",
            "Watch out: in sentence  23  contains  2  instances of the connective.\n",
            "Watch out: in sentence  38  contains  2  instances of the connective.\n",
            "Watch out: in sentence  39  contains  2  instances of the connective.\n",
            "Watch out: in sentence  43  contains  2  instances of the connective.\n",
            "Watch out: in sentence  44  contains  2  instances of the connective.\n",
            "Watch out: in sentence  111  contains  2  instances of the connective.\n",
            "Watch out: in sentence  112  contains  2  instances of the connective.\n",
            "Watch out: in sentence  153  contains  2  instances of the connective.\n",
            "Watch out: in sentence  154  contains  2  instances of the connective.\n",
            "Watch out: in sentence  280  contains  2  instances of the connective.\n",
            "Watch out: in sentence  281  contains  2  instances of the connective.\n",
            "Watch out: in sentence  443  contains  2  instances of the connective.\n",
            "Watch out: in sentence  444  contains  2  instances of the connective.\n",
            "\n",
            "For \"daarom\" holds the following warnings: \n",
            "\n",
            "For \"daardoor\" holds the following warnings: \n",
            "\n",
            "For \"omdat\" holds the following warnings: \n",
            "Watch out: in sentence  41  contains  2  instances of the connective.\n",
            "Watch out: in sentence  125  contains  2  instances of the connective.\n",
            "Watch out: in sentence  126  contains  2  instances of the connective.\n",
            "Watch out: in sentence  144  contains  2  instances of the connective.\n",
            "Watch out: in sentence  145  contains  2  instances of the connective.\n",
            "Watch out: in sentence  150  contains  2  instances of the connective.\n",
            "Watch out: in sentence  177  contains  2  instances of the connective.\n",
            "Watch out: in sentence  178  contains  3  instances of the connective.\n",
            "Watch out: in sentence  179  contains  2  instances of the connective.\n",
            "Watch out: in sentence  214  contains  2  instances of the connective.\n",
            "Watch out: in sentence  215  contains  2  instances of the connective.\n",
            "\n",
            "For \"want\" holds the following warnings: \n",
            "Watch out: in sentence  7  contains  2  instances of the connective.\n",
            "Watch out: in sentence  8  contains  2  instances of the connective.\n",
            "Watch out: in sentence  90  contains  2  instances of the connective.\n",
            "Watch out: in sentence  91  contains  2  instances of the connective.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2_XMdFGDXg3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "7a6255bf-9d2f-404f-fd08-d35fcbfe20c5"
      },
      "source": [
        "# Also creating vectors of the connectives in sentences without punctuation\n",
        "\n",
        "print('For \"dus\" holds the following warnings: ')\n",
        "dus_vecs_no_punctuation, dus_doubles_no_punctuation = find_vecs(encoded_sents_dus_no_punctuation, dus_default_dict_no_punctuation)\n",
        "\n",
        "print('\\nFor \"daarom\" holds the following warnings: ')\n",
        "daarom_vecs_no_punctuation, daarom_doubles_no_punctuation = find_vecs(encoded_sents_daarom_no_punctuation, daarom_default_dict_no_punctuation)\n",
        "\n",
        "print('\\nFor \"daardoor\" holds the following warnings: ')\n",
        "daardoor_vecs_no_punctuation, daardoor_doubles_no_punctuation = find_vecs(encoded_sents_daardoor_no_punctuation, daardoor_default_dict_no_punctuation)\n",
        "\n",
        "print('\\nFor \"omdat\" holds the following warnings: ')\n",
        "omdat_vecs_no_punctuation, omdat_doubles_no_punctuation = find_vecs(encoded_sents_omdat_no_punctuation, omdat_default_dict_no_punctuation)\n",
        "\n",
        "print('\\nFor \"want\" holds the following warnings: ')\n",
        "want_vecs_no_punctuation, want_doubles_no_punctuation = find_vecs(encoded_sents_want_no_punctuation, want_default_dict_no_punctuation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For \"dus\" holds the following warnings: \n",
            "Watch out: in sentence  5  contains  2  instances of the connective.\n",
            "Watch out: in sentence  14  contains  2  instances of the connective.\n",
            "Watch out: in sentence  15  contains  3  instances of the connective.\n",
            "Watch out: in sentence  16  contains  3  instances of the connective.\n",
            "Watch out: in sentence  17  contains  3  instances of the connective.\n",
            "Watch out: in sentence  18  contains  2  instances of the connective.\n",
            "Watch out: in sentence  22  contains  2  instances of the connective.\n",
            "Watch out: in sentence  23  contains  2  instances of the connective.\n",
            "Watch out: in sentence  38  contains  2  instances of the connective.\n",
            "Watch out: in sentence  39  contains  2  instances of the connective.\n",
            "Watch out: in sentence  43  contains  2  instances of the connective.\n",
            "Watch out: in sentence  44  contains  2  instances of the connective.\n",
            "Watch out: in sentence  111  contains  2  instances of the connective.\n",
            "Watch out: in sentence  112  contains  2  instances of the connective.\n",
            "Watch out: in sentence  153  contains  2  instances of the connective.\n",
            "Watch out: in sentence  154  contains  2  instances of the connective.\n",
            "Watch out: in sentence  280  contains  2  instances of the connective.\n",
            "Watch out: in sentence  281  contains  2  instances of the connective.\n",
            "Watch out: in sentence  443  contains  2  instances of the connective.\n",
            "Watch out: in sentence  444  contains  2  instances of the connective.\n",
            "\n",
            "For \"daarom\" holds the following warnings: \n",
            "\n",
            "For \"daardoor\" holds the following warnings: \n",
            "\n",
            "For \"omdat\" holds the following warnings: \n",
            "Watch out: in sentence  41  contains  2  instances of the connective.\n",
            "Watch out: in sentence  125  contains  2  instances of the connective.\n",
            "Watch out: in sentence  126  contains  2  instances of the connective.\n",
            "Watch out: in sentence  144  contains  2  instances of the connective.\n",
            "Watch out: in sentence  145  contains  2  instances of the connective.\n",
            "Watch out: in sentence  150  contains  2  instances of the connective.\n",
            "Watch out: in sentence  177  contains  2  instances of the connective.\n",
            "Watch out: in sentence  178  contains  3  instances of the connective.\n",
            "Watch out: in sentence  179  contains  2  instances of the connective.\n",
            "Watch out: in sentence  214  contains  2  instances of the connective.\n",
            "Watch out: in sentence  215  contains  2  instances of the connective.\n",
            "\n",
            "For \"want\" holds the following warnings: \n",
            "Watch out: in sentence  7  contains  2  instances of the connective.\n",
            "Watch out: in sentence  8  contains  2  instances of the connective.\n",
            "Watch out: in sentence  90  contains  2  instances of the connective.\n",
            "Watch out: in sentence  91  contains  2  instances of the connective.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7iSFmBeIaod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filtering out the double connectives (sadly no other option than to count by hand)\n",
        "# (I don't want the vectors of connectives in the context sentences, but only those from the target sentences)\n",
        "\n",
        "\n",
        "# indexes of vecs which need to be filtered out from dus_vecs\n",
        "dus_excess = [16, 17, 19, 20, 22, 23, 25, 26, 32, 33, 50, 51, 57, 58, 127, 128, 171, 172, 300, 301, 265, 266 ]\n",
        "omdat_excess = [127, 128, 148, 149, 184, 185, 187, 188, 225, 226 ]\n",
        "want_excess = [8, 9, 93, 94]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPywNGgAZtuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing excess vectors from vecs\n",
        "\n",
        "\n",
        "dus_vecs_new = []\n",
        "\n",
        "for i, vec in enumerate(dus_vecs):\n",
        "  if i not in dus_excess:\n",
        "    dus_vecs_new.append(vec)\n",
        "\n",
        "\n",
        "omdat_vecs_new = []\n",
        "\n",
        "for i, vec in enumerate(omdat_vecs):\n",
        "  if i not in omdat_excess:\n",
        "    omdat_vecs_new.append(vec)\n",
        "\n",
        "\n",
        "\n",
        "want_vecs_new = []\n",
        "\n",
        "for i, vec in enumerate(want_vecs):\n",
        "  if i not in want_excess:\n",
        "    want_vecs_new.append(vec)\n",
        "\n",
        "\n",
        "omdat_vecs = omdat_vecs_new\n",
        "want_vecs = want_vecs_new\n",
        "dus_vecs = dus_vecs_new\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB3IaFzeFaN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Doing the same for the sentences without punctuation\n",
        "\n",
        "dus_vecs_new_no_punctuation = []\n",
        "\n",
        "for i, vec in enumerate(dus_vecs_no_punctuation):\n",
        "  if i not in dus_excess:\n",
        "    dus_vecs_new_no_punctuation.append(vec)\n",
        "\n",
        "\n",
        "omdat_vecs_new_no_punctuation = []\n",
        "\n",
        "for i, vec in enumerate(omdat_vecs_no_punctuation):\n",
        "  if i not in omdat_excess:\n",
        "    omdat_vecs_new_no_punctuation.append(vec)\n",
        "\n",
        "\n",
        "\n",
        "want_vecs_new_no_punctuation = []\n",
        "\n",
        "for i, vec in enumerate(want_vecs_no_punctuation):\n",
        "  if i not in want_excess:\n",
        "    want_vecs_new_no_punctuation.append(vec)\n",
        "\n",
        "\n",
        "omdat_vecs_no_punctuation = omdat_vecs_new_no_punctuation\n",
        "want_vecs_no_punctuation = want_vecs_new_no_punctuation\n",
        "dus_vecs_no_punctuation = dus_vecs_new_no_punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP-UuT9Kb99K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating lists of sentences matching with the vecs, so target sentences with two vecs are doubled\n",
        "\n",
        "match_sents_dus = []\n",
        "\n",
        "for i, item in enumerate(dus_matches):\n",
        "  if i != 5:\n",
        "    match_sents_dus.append(item)\n",
        "  elif i == 5:\n",
        "    match_sents_dus.append(item)\n",
        "    match_sents_dus.append(item)\n",
        "\n",
        "match_sents_omdat = []\n",
        "\n",
        "for i, item in enumerate(omdat_matches):\n",
        "  if i != 41:\n",
        "    if i != 150:\n",
        "      match_sents_omdat.append(item)\n",
        "  if i == 41:\n",
        "    match_sents_omdat.append(item)\n",
        "    match_sents_omdat.append(item)\n",
        "  if i == 150:\n",
        "      match_sents_omdat.append(item)\n",
        "      match_sents_omdat.append(item)\n",
        "\n",
        "match_sents_daarom = daarom_matches\n",
        "match_sents_daardoor = daardoor_matches\n",
        "match_sents_want = want_matches\n",
        "\n",
        "\n",
        "match_sents_all = match_sents_dus + match_sents_daarom + match_sents_daardoor + match_sents_omdat + match_sents_want\n",
        "match_sents_dus_daarom_daardoor =  match_sents_dus + match_sents_daarom + match_sents_daardoor\n",
        "match_sents_omdat_want = match_sents_omdat + match_sents_want\n",
        "\n",
        "\n",
        "# Same matching sentences can be used for the vectors of the connectives in sentences without punctuation, therefore no repetition of this step\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ628ZUfgbf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "matching_sents_dus = [f'{i}: {sent}' for i, sent in enumerate(match_sents_dus)]\n",
        "matching_sents_daarom = [f'{i}: {sent}' for i, sent in enumerate(match_sents_daarom)]\n",
        "matching_sents_daardoor = [f'{i}: {sent}' for i, sent in enumerate(match_sents_daardoor)]\n",
        "matching_sents_want = [f'{i}: {sent}' for i, sent in enumerate(match_sents_want)]\n",
        "matching_sents_omdat = [f'{i}: {sent}' for i, sent in enumerate(match_sents_omdat)]\n",
        "\n",
        "matching_sents_all = [f'{i}: {sent}' for i, sent in enumerate(match_sents_all)]\n",
        "matching_sents_dus_daarom_daardoor =  [f'{i}: {sent}' for i, sent in enumerate(match_sents_dus_daarom_daardoor)]\n",
        "matching_sents_omdat_want = [f'{i}: {sent}' for i, sent in enumerate(match_sents_omdat_want)]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLS_K6fJ7gac",
        "colab_type": "text"
      },
      "source": [
        "## Turning the lists of vecs into 2D representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky6ooXmL9iLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# starting with necessary installations\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as PathEffects\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "sns.set_palette('muted')\n",
        "sns.set_context(\"notebook\", font_scale=1.5,\n",
        "                rc={\"lines.linewidth\": 2.5})\n",
        "RS = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnTISvkpA-Uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# turning the lists of vecs into data frames\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "dus_vecs_df = pd.DataFrame(dus_vecs)\n",
        "daarom_vecs_df = pd.DataFrame(daarom_vecs)\n",
        "daardoor_vecs_df = pd.DataFrame(daardoor_vecs)\n",
        "omdat_vecs_df = pd.DataFrame(omdat_vecs)\n",
        "want_vecs_df = pd.DataFrame(want_vecs)\n",
        "\n",
        "\n",
        "# Doing the same for the ones without punctuation\n",
        "\n",
        "dus_vecs_df_no_punctuation = pd.DataFrame(dus_vecs_no_punctuation)\n",
        "daarom_vecs_df_no_punctuation = pd.DataFrame(daarom_vecs_no_punctuation)\n",
        "daardoor_vecs_df_no_punctuation = pd.DataFrame(daardoor_vecs_no_punctuation)\n",
        "omdat_vecs_df_no_punctuation = pd.DataFrame(omdat_vecs_no_punctuation)\n",
        "want_vecs_df_no_punctuation = pd.DataFrame(want_vecs_no_punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ogOPd5DM3vI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combining vector lists for the combinations and turning those into data frames\n",
        "\n",
        "\n",
        "\n",
        "omdat_want_vecs = omdat_vecs + want_vecs\n",
        "dus_daarom_daardoor_vecs = dus_vecs + daarom_vecs + daardoor_vecs\n",
        "dus_daarom_daardoor_omdat_want_vecs = dus_daarom_daardoor_vecs + omdat_want_vecs\n",
        "\n",
        "\n",
        "omdat_want_vecs_df = pd.DataFrame(omdat_want_vecs)\n",
        "dus_daarom_daardoor_vecs_df = pd.DataFrame(dus_daarom_daardoor_vecs)\n",
        "dus_daarom_daardoor_omdat_want_vecs_df = pd.DataFrame(dus_daarom_daardoor_omdat_want_vecs)\n",
        "\n",
        "\n",
        "# Doing the same for the ones without punctuation\n",
        "\n",
        "omdat_want_vecs_no_punctuation = omdat_vecs_no_punctuation + want_vecs_no_punctuation\n",
        "dus_daarom_daardoor_vecs_no_punctuation = dus_vecs_no_punctuation + daarom_vecs_no_punctuation + daardoor_vecs_no_punctuation\n",
        "dus_daarom_daardoor_omdat_want_vecs_no_punctuation = dus_daarom_daardoor_vecs_no_punctuation + omdat_want_vecs_no_punctuation\n",
        "\n",
        "\n",
        "omdat_want_vecs_df_no_punctuation = pd.DataFrame(omdat_want_vecs_no_punctuation)\n",
        "dus_daarom_daardoor_vecs_df_no_punctuation = pd.DataFrame(dus_daarom_daardoor_vecs_no_punctuation)\n",
        "dus_daarom_daardoor_omdat_want_vecs_df_no_punctuation = pd.DataFrame(dus_daarom_daardoor_omdat_want_vecs_no_punctuation)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5sYTog39oL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for the plot\n",
        "\n",
        "def labeled_scatter(x, labels, size):\n",
        "  \n",
        "  # choose a color palette with seaborn.\n",
        "  num_classes = len(np.unique(labels))\n",
        "  print(num_classes) # kan straks weg\n",
        "  palette = np.array(sns.color_palette(\"hls\", num_classes))\n",
        "\n",
        "  colors = np.zeros(len(labels), dtype= object)\n",
        "  for i, label in enumerate(set(labels)):\n",
        "    colors[labels == label] = f'C{i}'\n",
        "\n",
        "  # create a scatter plot.\n",
        "  f = plt.figure(figsize=(size, size))\n",
        "  ax = plt.subplot(aspect='equal')\n",
        "  sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=colors)\n",
        "  plt.xlim(-25, 25)\n",
        "  plt.ylim(-25, 25)\n",
        "  ax.axis('off')\n",
        "  ax.axis('tight')\n",
        "\n",
        "  # add the labels for each digit corresponding to the label\n",
        "  txts = []\n",
        "\n",
        "  for i in range(len(x)):\n",
        "\n",
        "    # Position of each label at median of data po\n",
        "\n",
        "    plt.annotate(i, xy = (x[i,0], x[i,1]))\n",
        "\n",
        "  return f, ax, sc, txts\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O9HznvVHIBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating color coordinated labels\n",
        "\n",
        "# seperate\n",
        "labels_dus = ['dus'] * len(dus_vecs)\n",
        "labels_daarom = ['daarom'] * len(daarom_vecs)\n",
        "labels_daardoor = ['daardoor'] * len(daardoor_vecs)\n",
        "labels_omdat = ['omdat'] * len(omdat_vecs)\n",
        "labels_want = ['want'] * len(want_vecs)\n",
        "\n",
        "# combinations\n",
        "labels_omdat_want = labels_omdat + labels_want\n",
        "labels_dus_daarom_daardoor = labels_dus + labels_daarom + labels_daardoor\n",
        "labels_dus_daarom_daardoor_omdat_want = labels_dus_daarom_daardoor + labels_omdat_want\n",
        "\n",
        "# putting the labels in the right format\n",
        "labels_dus_pd = pd.Series(labels_dus)\n",
        "labels_daarom_pd = pd.Series(labels_daarom)\n",
        "labels_daardoor_pd = pd.Series(labels_daardoor)\n",
        "labels_omdat_pd = pd.Series(labels_omdat)\n",
        "labels_want_pd = pd.Series(labels_want)\n",
        "\n",
        "labels_omdat_want_pd = pd.Series(labels_omdat_want)\n",
        "labels_dus_daarom_daardoor_pd = pd.Series(labels_dus_daarom_daardoor)\n",
        "labels_dus_daarom_daardoor_omdat_want_pd = pd.Series(labels_dus_daarom_daardoor_omdat_want)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rNJ1mZRMC6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Applying PCA\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "dus_vecs_pca = PCA(n_components=2).fit_transform(dus_vecs_df.values)\n",
        "daarom_vecs_pca = PCA(n_components=2).fit_transform(daarom_vecs_df.values)\n",
        "daardoor_vecs_pca = PCA(n_components=2).fit_transform(daardoor_vecs_df.values)\n",
        "omdat_vecs_pca = PCA(n_components=2).fit_transform(omdat_vecs_df.values)\n",
        "want_vecs_pca = PCA(n_components=2).fit_transform(want_vecs_df.values)\n",
        "\n",
        "omdat_want_vecs_pca = PCA(n_components=2).fit_transform(omdat_want_vecs_df.values)\n",
        "dus_daarom_daardoor_vecs_pca = PCA(n_components=2).fit_transform(dus_daarom_daardoor_vecs_df.values)\n",
        "dus_daarom_daardoor_omdat_want_vecs_pca = PCA(n_components=2).fit_transform(dus_daarom_daardoor_omdat_want_vecs_df.values)\n",
        "\n",
        "\n",
        "\n",
        "# Doing the same for the ones without punctuation\n",
        "\n",
        "dus_vecs_pca_no_punctuation = PCA(n_components=2).fit_transform(dus_vecs_df_no_punctuation.values)\n",
        "daarom_vecs_pca_no_punctuation = PCA(n_components=2).fit_transform(daarom_vecs_df_no_punctuation.values)\n",
        "daardoor_vecs_pca_no_punctuation = PCA(n_components=2).fit_transform(daardoor_vecs_df_no_punctuation.values)\n",
        "omdat_vecs_pca_no_punctuation = PCA(n_components=2).fit_transform(omdat_vecs_df_no_punctuation.values)\n",
        "want_vecs_pca_no_punctuation = PCA(n_components=2).fit_transform(want_vecs_df_no_punctuation.values)\n",
        "\n",
        "omdat_want_vecs_pca_no_punctuation = PCA(n_components=2).fit_transform(omdat_want_vecs_df_no_punctuation.values)\n",
        "dus_daarom_daardoor_vecs_pca_no_punctuation = PCA(n_components=2).fit_transform(dus_daarom_daardoor_vecs_df_no_punctuation.values)\n",
        "dus_daarom_daardoor_omdat_want_vecs_pca_no_punctuation = PCA(n_components=2).fit_transform(dus_daarom_daardoor_omdat_want_vecs_df_no_punctuation.values)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxSB_WIpnQOp",
        "colab_type": "text"
      },
      "source": [
        "## Altair plots\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t75rPSB83BeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://matthewkudija.com/blog/2018/06/22/altair-interactive/\n",
        "\n",
        "import altair as alt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIRz7Efp881K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add pca columns\n",
        "\n",
        "\n",
        "#connectives solo\n",
        "dus_vecs_df['PCA1'] = dus_vecs_pca[:,0]\n",
        "dus_vecs_df['PCA2'] = dus_vecs_pca[:,1]\n",
        "\n",
        "daarom_vecs_df['PCA1'] = daarom_vecs_pca[:,0]\n",
        "daarom_vecs_df['PCA2'] = daarom_vecs_pca[:,1]\n",
        "\n",
        "daardoor_vecs_df['PCA1'] = daardoor_vecs_pca[:,0]\n",
        "daardoor_vecs_df['PCA2'] = daardoor_vecs_pca[:,1]\n",
        "\n",
        "omdat_vecs_df['PCA1'] = omdat_vecs_pca[:,0]\n",
        "omdat_vecs_df['PCA2'] = omdat_vecs_pca[:,1]\n",
        "\n",
        "want_vecs_df['PCA1'] = want_vecs_pca[:,0]\n",
        "want_vecs_df['PCA2'] = want_vecs_pca[:,1]\n",
        "\n",
        "\n",
        "#connectives combined\n",
        "dus_daarom_daardoor_vecs_df['PCA1'] = dus_daarom_daardoor_vecs_pca[:,0]\n",
        "dus_daarom_daardoor_vecs_df['PCA2'] = dus_daarom_daardoor_vecs_pca[:,1]\n",
        "\n",
        "omdat_want_vecs_df['PCA1'] = omdat_want_vecs_pca[:,0]\n",
        "omdat_want_vecs_df['PCA2'] = omdat_want_vecs_pca[:,1]\n",
        "\n",
        "dus_daarom_daardoor_omdat_want_vecs_df['PCA1'] = dus_daarom_daardoor_omdat_want_vecs_pca[:,0]\n",
        "dus_daarom_daardoor_omdat_want_vecs_df['PCA2'] = dus_daarom_daardoor_omdat_want_vecs_pca[:,1]\n",
        "\n",
        "\n",
        "\n",
        "# Doing the same for the ones without punctuation\n",
        "\n",
        "#connectives solo\n",
        "dus_vecs_df_no_punctuation['PCA1'] = dus_vecs_pca_no_punctuation[:,0]\n",
        "dus_vecs_df_no_punctuation['PCA2'] = dus_vecs_pca_no_punctuation[:,1]\n",
        "\n",
        "daarom_vecs_df_no_punctuation['PCA1'] = daarom_vecs_pca_no_punctuation[:,0]\n",
        "daarom_vecs_df_no_punctuation['PCA2'] = daarom_vecs_pca_no_punctuation[:,1]\n",
        "\n",
        "daardoor_vecs_df_no_punctuation['PCA1'] = daardoor_vecs_pca_no_punctuation[:,0]\n",
        "daardoor_vecs_df_no_punctuation['PCA2'] = daardoor_vecs_pca_no_punctuation[:,1]\n",
        "\n",
        "omdat_vecs_df_no_punctuation['PCA1'] = omdat_vecs_pca_no_punctuation[:,0]\n",
        "omdat_vecs_df_no_punctuation['PCA2'] = omdat_vecs_pca_no_punctuation[:,1]\n",
        "\n",
        "want_vecs_df_no_punctuation['PCA1'] = want_vecs_pca_no_punctuation[:,0]\n",
        "want_vecs_df_no_punctuation['PCA2'] = want_vecs_pca_no_punctuation[:,1]\n",
        "\n",
        "\n",
        "#connectives combined\n",
        "dus_daarom_daardoor_vecs_df_no_punctuation['PCA1'] = dus_daarom_daardoor_vecs_pca_no_punctuation[:,0]\n",
        "dus_daarom_daardoor_vecs_df_no_punctuation['PCA2'] = dus_daarom_daardoor_vecs_pca_no_punctuation[:,1]\n",
        "\n",
        "omdat_want_vecs_df_no_punctuation['PCA1'] = omdat_want_vecs_pca_no_punctuation[:,0]\n",
        "omdat_want_vecs_df_no_punctuation['PCA2'] = omdat_want_vecs_pca_no_punctuation[:,1]\n",
        "\n",
        "dus_daarom_daardoor_omdat_want_vecs_df_no_punctuation['PCA1'] = dus_daarom_daardoor_omdat_want_vecs_pca_no_punctuation[:,0]\n",
        "dus_daarom_daardoor_omdat_want_vecs_df_no_punctuation['PCA2'] = dus_daarom_daardoor_omdat_want_vecs_pca_no_punctuation[:,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX4M0RZaJCVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turn columns into strings\n",
        "\n",
        "#connectives solo\n",
        "dus_vecs_df.columns = dus_vecs_df.columns.astype(str)\n",
        "daarom_vecs_df.columns = daarom_vecs_df.columns.astype(str)\n",
        "daardoor_vecs_df.columns = daardoor_vecs_df.columns.astype(str)\n",
        "omdat_vecs_df.columns = omdat_vecs_df.columns.astype(str)\n",
        "want_vecs_df.columns = want_vecs_df.columns.astype(str)\n",
        "\n",
        "#connectives combined\n",
        "dus_daarom_daardoor_vecs_df.columns = dus_daarom_daardoor_vecs_df.columns.astype(str)\n",
        "omdat_want_vecs_df.columns = omdat_want_vecs_df.columns.astype(str)\n",
        "dus_daarom_daardoor_omdat_want_vecs_df.columns = dus_daarom_daardoor_omdat_want_vecs_df.columns.astype(str)\n",
        "\n",
        "\n",
        "\n",
        "# Doing the same for the ones without punctuation\n",
        "\n",
        "#connectives solo\n",
        "dus_vecs_df_no_punctuation.columns = dus_vecs_df_no_punctuation.columns.astype(str)\n",
        "daarom_vecs_df_no_punctuation.columns = daarom_vecs_df_no_punctuation.columns.astype(str)\n",
        "daardoor_vecs_df_no_punctuation.columns = daardoor_vecs_df_no_punctuation.columns.astype(str)\n",
        "omdat_vecs_df_no_punctuation.columns = omdat_vecs_df_no_punctuation.columns.astype(str)\n",
        "want_vecs_df_no_punctuation.columns = want_vecs_df_no_punctuation.columns.astype(str)\n",
        "\n",
        "#connectives combined\n",
        "dus_daarom_daardoor_vecs_df_no_punctuation.columns = dus_daarom_daardoor_vecs_df_no_punctuation.columns.astype(str)\n",
        "omdat_want_vecs_df_no_punctuation.columns = omdat_want_vecs_df_no_punctuation.columns.astype(str)\n",
        "dus_daarom_daardoor_omdat_want_vecs_df_no_punctuation.columns = dus_daarom_daardoor_omdat_want_vecs_df_no_punctuation.columns.astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maNYuaQOKSdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add columns containing the matching sentences and the labels\n",
        "\n",
        "# connectives solo\n",
        "dus_vecs_df['Sentence'] = matching_sents_dus\n",
        "dus_vecs_df['Connectives'] = labels_dus\n",
        "\n",
        "daarom_vecs_df['Sentence'] = matching_sents_daarom\n",
        "daarom_vecs_df['Connectives'] = labels_daarom\n",
        "\n",
        "daardoor_vecs_df['Sentence'] = matching_sents_daardoor\n",
        "daardoor_vecs_df['Connectives'] = labels_daardoor\n",
        "\n",
        "omdat_vecs_df['Sentence'] = matching_sents_omdat\n",
        "omdat_vecs_df['Connectives'] = labels_omdat\n",
        "\n",
        "want_vecs_df['Sentence'] = matching_sents_want\n",
        "want_vecs_df['Connectives'] = labels_want\n",
        "\n",
        "\n",
        "# connectives combined\n",
        "dus_daarom_daardoor_vecs_df['Sentence'] = matching_sents_dus_daarom_daardoor\n",
        "dus_daarom_daardoor_vecs_df['Connectives'] = labels_dus_daarom_daardoor\n",
        "\n",
        "omdat_want_vecs_df['Sentence'] = matching_sents_omdat_want\n",
        "omdat_want_vecs_df['Connectives'] = labels_omdat_want\n",
        "\n",
        "dus_daarom_daardoor_omdat_want_vecs_df['Sentence'] = matching_sents_all\n",
        "dus_daarom_daardoor_omdat_want_vecs_df['Connectives'] = labels_dus_daarom_daardoor_omdat_want\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Doing the same for the ones without punctuation (same labels and sentences as the regular ones though)\n",
        "\n",
        "# connectives solo\n",
        "dus_vecs_df_no_punctuation['Sentence'] = matching_sents_dus\n",
        "dus_vecs_df_no_punctuation['Connectives'] = labels_dus\n",
        "\n",
        "daarom_vecs_df_no_punctuation['Sentence'] = matching_sents_daarom\n",
        "daarom_vecs_df_no_punctuation['Connectives'] = labels_daarom\n",
        "\n",
        "daardoor_vecs_df_no_punctuation['Sentence'] = matching_sents_daardoor\n",
        "daardoor_vecs_df_no_punctuation['Connectives'] = labels_daardoor\n",
        "\n",
        "omdat_vecs_df_no_punctuation['Sentence'] = matching_sents_omdat\n",
        "omdat_vecs_df_no_punctuation['Connectives'] = labels_omdat\n",
        "\n",
        "want_vecs_df_no_punctuation['Sentence'] = matching_sents_want\n",
        "want_vecs_df_no_punctuation['Connectives'] = labels_want\n",
        "\n",
        "\n",
        "# connectives combined\n",
        "dus_daarom_daardoor_vecs_df_no_punctuation['Sentence'] = matching_sents_dus_daarom_daardoor\n",
        "dus_daarom_daardoor_vecs_df_no_punctuation['Connectives'] = labels_dus_daarom_daardoor\n",
        "\n",
        "omdat_want_vecs_df_no_punctuation['Sentence'] = matching_sents_omdat_want\n",
        "omdat_want_vecs_df_no_punctuation['Connectives'] = labels_omdat_want\n",
        "\n",
        "dus_daarom_daardoor_omdat_want_vecs_df_no_punctuation['Sentence'] = matching_sents_all\n",
        "dus_daarom_daardoor_omdat_want_vecs_df_no_punctuation['Connectives'] = labels_dus_daarom_daardoor_omdat_want"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e3xvUhzY72V",
        "colab_type": "text"
      },
      "source": [
        "# plotting the connectives individually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqXHSG4MUkjT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "outputId": "333d6fd9-f64c-4fe1-da39-577d4f75f412"
      },
      "source": [
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "wd.get(\"https://www.webite-url.com\")\n",
        "\n",
        "## This was necessary for saving svg files!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.6/dist-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:2 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Hit:6 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Ign:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [103 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,303 kB]\n",
            "Fetched 1,658 kB in 2s (1,042 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (83.0.4103.61-0ubuntu0.18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 78 not upgraded.\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: use options instead of chrome_options\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lZyiUyoJOF3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "4ae3a1c7-47c1-45e5-ebe3-fbee9cd57801"
      },
      "source": [
        "!pip install altair_saver"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: altair_saver in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: altair-viewer in /usr/local/lib/python3.6/dist-packages (from altair_saver) (0.3.0)\n",
            "Requirement already satisfied: altair-data-server>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from altair_saver) (0.4.1)\n",
            "Requirement already satisfied: altair in /usr/local/lib/python3.6/dist-packages (from altair_saver) (4.1.0)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.6/dist-packages (from altair_saver) (3.141.0)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.6/dist-packages (from altair-data-server>=0.4.0->altair_saver) (1.3.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from altair-data-server>=0.4.0->altair_saver) (4.5.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from altair->altair_saver) (2.11.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from altair->altair_saver) (2.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from altair->altair_saver) (1.18.5)\n",
            "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.6/dist-packages (from altair->altair_saver) (1.0.5)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.6/dist-packages (from altair->altair_saver) (0.10.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from altair->altair_saver) (0.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium->altair_saver) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->altair->altair_saver) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18->altair->altair_saver) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18->altair->altair_saver) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.18->altair->altair_saver) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJdaYeVmHrAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from altair_saver import save\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkb4APYmOczP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dus plot\n",
        "\n",
        "dus_plot = alt.Chart(dus_vecs_df).mark_point().encode(\n",
        "           x = alt.X('PCA1', axis=alt.Axis(labels=False, title = 'Dus')),\n",
        "           y = alt.Y('PCA2', axis=alt.Axis(labels=False, title = '')),\n",
        "           color = 'Connectives',\n",
        "           tooltip = ['Sentence']).interactive()\n",
        "\n",
        "dus_plot.save('dus_plot.html')\n",
        "\n",
        "\n",
        "# Without punctuation\n",
        "\n",
        "dus_plot_no_punctuation = alt.Chart(dus_vecs_df_no_punctuation).mark_point().encode(\n",
        "           x = alt.X('PCA1', axis=alt.Axis(labels=False, title = 'Dus without punctuation taken into account')),\n",
        "           y = alt.Y('PCA2', axis=alt.Axis(labels=False, title = '')),\n",
        "           color = 'Connectives',\n",
        "           tooltip = ['Sentence']).interactive()\n",
        "\n",
        "dus_plot_no_punctuation.save('dus_plot_no_punctuation.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHxAxkz4ZB1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# daarom plot\n",
        "\n",
        "daarom_plot = alt.Chart(daarom_vecs_df).mark_point().encode(\n",
        "           x = alt.X('PCA1', axis=alt.Axis(labels=False, title = 'Daarom')),\n",
        "           y = alt.Y('PCA2', axis=alt.Axis(labels=False, title = '')),\n",
        "           color = 'Connectives',\n",
        "           tooltip = ['Sentence']).interactive()\n",
        "\n",
        "daarom_plot.save('daarom_plot.html')\n",
        "\n",
        "\n",
        "# Without punctuation\n",
        "\n",
        "daarom_plot_no_punctuation = alt.Chart(daarom_vecs_df_no_punctuation).mark_point().encode(\n",
        "           x = alt.X('PCA1', axis=alt.Axis(labels=False, title = 'Daarom without punctuation taken into account')),\n",
        "           y = alt.Y('PCA2', axis=alt.Axis(labels=False, title = '')),\n",
        "           color = 'Connectives',\n",
        "           tooltip = ['Sentence']).interactive()\n",
        "\n",
        "daarom_plot_no_punctuation.save('daarom_plot_no_punctuation.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eabSjxXDZEVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# daardoor plot\n",
        "\n",
        "daardoor_plot = alt.Chart(daardoor_vecs_df).mark_point().encode(\n",
        "           x = alt.X('PCA1', axis=alt.Axis(labels=False, title = 'Daardoor')),\n",
        "           y = alt.Y('PCA2', axis=alt.Axis(labels=False, title = '')),\n",
        "           color = 'Connectives',\n",
        "           tooltip = ['Sentence']).interactive()\n",
        "\n",
        "daardoor_plot.save('daardoor_plot.html')\n",
        "\n",
        "\n",
        "# Without punctuation\n",
        "\n",
        "daardoor_plot_no_punctuation = alt.Chart(daardoor_vecs_df_no_punctuation).mark_point().encode(\n",
        "           x = alt.X('PCA1', axis=alt.Axis(labels=False, title = 'Daardoor without punctuation taken into account')),\n",
        "           y = alt.Y('PCA2', axis=alt.Axis(labels=False, title = '')),\n",
        "           color = 'Connectives',\n",
        "           tooltip = ['Sentence']).interactive()\n",
        "\n",
        "daardoor_plot_no_punctuation.save('daardoor_plot_no_punctuation.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SGUFOYtZE0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# omdat plot\n",
        "\n",
        "omdat_plot = alt.Chart(omdat_vecs_df).mark_point().encode(\n",
        "           x = alt.X('PCA1', axis=alt.Axis(labels=False, title = 'Omdat')),\n",
        "           y = alt.Y('PCA2', axis=alt.Axis(labels=False, title = '' )),\n",
        "           color = 'Connectives',\n",
        "           tooltip = ['Sentence']).interactive()\n",
        "\n",
        "omdat_plot.save('omdat_plot.html')\n",
        "\n",
        "# Without punctuation\n",
        "\n",
        "omdat_plot_no_punctuation = alt.Chart(omdat_vecs_df_no_punctuation).mark_point().encode(\n",
        "           x = alt.X('PCA1', axis=alt.Axis(labels=False, title = 'Omdat with no punctuation taken into account')),\n",
        "           y = alt.Y('PCA2', axis=alt.Axis(labels=False, title = '' )),\n",
        "           color = 'Connectives',\n",
        "           tooltip = ['Sentence']).interactive()\n",
        "\n",
        "omdat_plot_no_punctuation.save('omdat_plot_no_punctuation.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y8tlX-QZFWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# want plot\n",
        "\n",
        "want_plot = alt.Chart(want_vecs_df).mark_point().encode(\n",
        "           x = alt.X('PCA1', axis=alt.Axis(labels=False, title = 'Want')),\n",
        "           y = alt.Y('PCA2', axis=alt.Axis(labels=False, title = '')),\n",
        "           color = 'Connectives',\n",
        "           tooltip = ['Sentence']).interactive()\n",
        "\n",
        "want_plot.save('want_plot.html')\n",
        "\n",
        "\n",
        "\n",
        "# Without punctuation\n",
        "\n",
        "want_plot_no_punctuation = alt.Chart(want_vecs_df_no_punctuation).mark_point().encode(\n",
        "           x = alt.X('PCA1', axis=alt.Axis(labels=False, title = 'Want without punctuation taken into account')),\n",
        "           y = alt.Y('PCA2', axis=alt.Axis(labels=False, title = '')),\n",
        "           color = 'Connectives',\n",
        "           tooltip = ['Sentence']).interactive()\n",
        "\n",
        "want_plot_no_punctuation.save('want_plot_no_punctuation.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAHyXVljZYmz",
        "colab_type": "text"
      },
      "source": [
        "# Plotting combinations of the connectives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFYTZ5twZnRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dus, daarom, and daardoor plot\n",
        "\n",
        "dus_daarom_daardoor_plot = alt.Chart(dus_daarom_daardoor_vecs_df).mark_point().encode(\n",
        "            x = alt.X('PCA1', axis=alt.Axis(labels=False, title='Dus, daarom and daardoor')),\n",
        "           y = alt.Y('PCA2', axis=alt.Axis(labels=False, title= '')),\n",
        "           color = 'Connectives',\n",
        "           tooltip = ['Sentence']).interactive()\n",
        "\n",
        "dus_daarom_daardoor_plot.save('dus_daarom_daardoor_plot.html')\n",
        "\n",
        "\n",
        "# Without punctuation\n",
        "\n",
        "dus_daarom_daardoor_plot_no_punctuation = alt.Chart(dus_daarom_daardoor_vecs_df_no_punctuation).mark_point().encode(\n",
        "            x = alt.X('PCA1', axis=alt.Axis(labels=False, title='Dus, daarom and daardoor without punctuation taken into account')),\n",
        "           y = alt.Y('PCA2', axis=alt.Axis(labels=False, title= '')),\n",
        "           color = 'Connectives',\n",
        "           tooltip = ['Sentence']).interactive()\n",
        "\n",
        "dus_daarom_daardoor_plot_no_punctuation.save('dus_daarom_daardoor_plot_no_punctuation.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpinMOxvSJBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# omdat and want plot\n",
        "\n",
        "omdat_want_plot = alt.Chart(omdat_want_vecs_df).mark_point().encode(\n",
        "           x = alt.X('PCA1', axis=alt.Axis(labels=False, title= 'Omdat and want')),\n",
        "           y = alt.Y('PCA2', axis=alt.Axis(labels=False, title = '')),\n",
        "           color = 'Connectives',\n",
        "           tooltip = ['Sentence']).interactive()\n",
        "\n",
        "omdat_want_plot.save('omdat_want_plot.html')\n",
        "\n",
        "\n",
        "# Without punctuation\n",
        "\n",
        "omdat_want_plot_no_punctuation = alt.Chart(omdat_want_vecs_df_no_punctuation).mark_point().encode(\n",
        "           x = alt.X('PCA1', axis=alt.Axis(labels=False, title= 'Omdat and want without punctuation taken into account')),\n",
        "           y = alt.Y('PCA2', axis=alt.Axis(labels=False, title = '')),\n",
        "           color = 'Connectives',\n",
        "           tooltip = ['Sentence']).interactive()\n",
        "\n",
        "omdat_want_plot_no_punctuation.save('omdat_want_plot_no_punctuation.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujc1EXL6ZyL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# all connectives combined plot\n",
        "\n",
        "all_connectives_plot = alt.Chart(dus_daarom_daardoor_omdat_want_vecs_df).mark_point().encode(\n",
        "           x = alt.X('PCA1', axis=alt.Axis(labels=False, title = 'All connectives combined')),\n",
        "           y = alt.Y('PCA2', axis=alt.Axis(labels=False, title = '')),\n",
        "           color = 'Connectives',\n",
        "           tooltip = ['Sentence']).interactive()\n",
        "\n",
        "all_connectives_plot.save('all_connectives_plot.html')\n",
        "\n",
        "\n",
        "# Without punctuation\n",
        "\n",
        "all_connectives_plot_no_punctuation = alt.Chart(dus_daarom_daardoor_omdat_want_vecs_df_no_punctuation).mark_point().encode(\n",
        "           x = alt.X('PCA1', axis=alt.Axis(labels=False, title = 'All connectives combined without punctuation taken into account')),\n",
        "           y = alt.Y('PCA2', axis=alt.Axis(labels=False, title = '')),\n",
        "           color = 'Connectives',\n",
        "           tooltip = ['Sentence']).interactive()\n",
        "\n",
        "all_connectives_plot_no_punctuation.save('all_connectives_plot_no_punctuation.html')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}